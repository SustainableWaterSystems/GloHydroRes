{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign hydropower plant to their nearest reservoir\n",
    "\n",
    "Author : Jignesh Shah, Jing Hu, Oreane Edlenbosch and Michelle T.H van Vliet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "Following packages are required\n",
    "\n",
    "1. xarray\n",
    "2. rioxarray\n",
    "3. numpy\n",
    "4. geopands\n",
    "5. shapely\n",
    "6. geopy\n",
    "7. pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files required to be downloaded\n",
    "\n",
    "The GloHydroRes dataset is a compilation of open-source hydropower plant and reservoir data. It combines information about hydropower plants (such as location, capacity, water head, and type) with reservoir characteristics (including dam and reservoir location, dam height, reservoir depth, area, and volume). Below mentioned datasets are used in GloHydroRes dataset.\n",
    "\n",
    "1. Global Power Plant Database from World Resource Institute (WRI) [https://datasets.wri.org/dataset/globalpowerplantdatabase]\n",
    "2. Hydropower Infrastruture - LAkes, Reservoirs, and RIvers (HILARRI), Version 2 [https://hydrosource.ornl.gov/dataset/hilarri-v2]\n",
    "3. Existing Hydropower Assets (EHA) Plant Database, 2022 [https://hydrosource.ornl.gov/dataset/EHA2022]\n",
    "4. JRC Hydro-power plants database [https://github.com/energy-modelling-toolkit/hydro-power-database/]\n",
    "5. RePP Africa - Renewable Power Plant database for Africa [https://www.nature.com/articles/s41597-022-01922-1]\n",
    "6. Global Reservoir and Dam (GranD) database [https://depts.washington.edu/saswe/grand/GRanD_Technical_Documentation_v1_1.pdf]\n",
    "6. Georeferenced global Dams and Reservoir (GeoDAR) [https://essd.copernicus.org/articles/14/1869/2022/]\n",
    "7. Global Dam Tracker (GDAT) [https://www.nature.com/articles/s41597-023-02008-2]\n",
    "8. HydroLAKES [https://www.hydrosheds.org/products/hydrolakes]\n",
    "9. HydroSHEDs 15 arc-second DEM is used to determine the elevation of hydropower plants and dams. It can be download from https://www.hydrosheds.org/hydrosheds-core-downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "from rioxarray import open_rasterio\n",
    "from geopandas import read_file\n",
    "from xarray import open_dataset\n",
    "from shapely.ops import nearest_points # For finding the nearest point\n",
    "from geopy.distance import geodesic # For calculating the distance between two points (coordinates)\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from shapely.geometry import Point\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_data(folder_name, file_type, file_name):\n",
    "    \"\"\" Load the input data\n",
    "\n",
    "    : param folder_type: str: folder name of the input data\n",
    "    : param file_type: str: type of the input data\n",
    "    : param file_name: str: file name of the input data\n",
    "    : return data: pd.DataFrame, xr.Dataset, gpd.GeoDataFrame: input data\n",
    "    \n",
    "    \"\"\"\n",
    "    # get path to the input data\n",
    "\n",
    "    input_path = f\"{folder_name}/{file_name}\"\n",
    "\n",
    "    if file_type == \"tif\":\n",
    "        data = open_rasterio(input_path)\n",
    "        data =  data.isel(band=0)\n",
    "\n",
    "    elif file_type == \"shp\":\n",
    "        data = read_file(input_path)\n",
    "    \n",
    "    elif file_type == \"nc\":\n",
    "        data = open_dataset(input_path)\n",
    "    \n",
    "    elif file_type == \"csv\":\n",
    "        data = pd.read_csv(input_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"File type {file_type} is not supported\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hydropower following datasets were used : \n",
    "1) WRI \n",
    "2) EHA \n",
    "3) RePP\n",
    "4) JRC\n",
    "\n",
    "For Reservoir following datasets were used:  \n",
    "1) GranD\n",
    "2) GeoDAR \n",
    "3) HydroLakes\n",
    "\n",
    "Unique id used in each dataset\n",
    "1) WRI - gppd_idnr\n",
    "2) RePP - ID\n",
    "3) JRC - id\n",
    "4) EHA - eha_ptid\n",
    "5) GranD - grand_id\n",
    "6) GeoDAR - id_v11\n",
    "7) HydroLakes - Hylak_id\n",
    "\n",
    "\n",
    "Columns selected from each hydropower dataset\n",
    "1) WRI - country_long, name, capacity_mw, latitude, longitude, gppd_idnr, commissioning_year\n",
    "2) JRC - id, name, installed_capacity_MW, type, lat, lon, dam_height_m, volume_Mm3, country\n",
    "3) EHA and HILARRI - EHA_PtID, MW, Head_ft, OpYear, grand_id, pt_name, pt_lat, pt_lon\n",
    "4) RePP - HPPD_ID, lon, lat, country, HPP_name, type, main_river, g_hgt_m, first_o_start, g_cap_mw\n",
    "\n",
    "Columns selected from each reservoir dataset\n",
    "1) GranD - GRAND_ID, RES_NAME, DAM_NAME, RIVER, DAM_HGT_M, AREA_SKM (Representative surface area of reservoir in km2), CAP_MCM (Representative maximum storage capacity of reservoir in million cubic meters), DEPTH_M, \n",
    "2) GeoDAR - id_v11\n",
    "3) HydroLakes - Hylak_id, Lake_name, Lake_area, Vol_total and Depth_avg\n",
    "3) GDAT - Area_Con (Surface area of the reservoir in square kilometers, consolidated from polygon, official, max, min), Volume_Con (Maximum storage capacity of the reservoir in million cubic meters, consolidated from official, max, min), Avg_Depth, Dam_Name, Reservoir, River, Height\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shah0012/.conda/envs/python_env/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Open WRI data.\n",
    "WRI_power_plant = load_input_data(\"/home/shah0012/GloHydroRes/Input_data/world_resource_institute_data/\", \"csv\", \"global_power_plant_database.csv\")\n",
    "WRI_hydro = WRI_power_plant[WRI_power_plant.primary_fuel == \"Hydro\"]\n",
    "WRI_hydro = gpd.GeoDataFrame(WRI_hydro, geometry=gpd.points_from_xy(WRI_hydro.longitude, WRI_hydro.latitude), crs=\"EPSG:4326\")\n",
    "WRI_hydro = WRI_hydro[[\"country_long\", \"name\", \"capacity_mw\", \"latitude\", \"longitude\", \"gppd_idnr\", \"commissioning_year\"]]\n",
    "WRI_hydro.rename(columns={\"country_long\": \"country\", \"latitude\": \"plant_lat\", \"longitude\": \"plant_lon\", \"gppd_idnr\": \"plant_source_id\", \"commissioning_year\" : \"year\"}, inplace=True)\n",
    "WRI_hydro[\"plant_source\"] = \"WRI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shah0012/.conda/envs/python_env/lib/python3.7/site-packages/pyproj/crs/crs.py:131: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "# Open JRC hydropower data\n",
    "JRC_hydropower_data = pd.read_csv(\"https://raw.githubusercontent.com/energy-modelling-toolkit/hydro-power-database/master/data/jrc-hydro-power-plant-database.csv\", sep=\",\")\n",
    "geometry = [Point(xy) for xy in zip(JRC_hydropower_data.lon, JRC_hydropower_data.lat)]\n",
    "crs = {'init': 'epsg:4326'}\n",
    "JRC_hydropower_data = gpd.GeoDataFrame(JRC_hydropower_data, crs=crs, geometry=geometry)\n",
    "\n",
    "\n",
    "# Convert volume from Mm3 to km3\n",
    "JRC_hydropower_data[\"res_vol_km3\"] = JRC_hydropower_data.volume_Mm3*0.001\n",
    "\n",
    "\n",
    "# Country code to country name. JRC does not provide country name instead it provides country code\n",
    "country_code_name_dict = {'CH' : \"Switzerland\",\n",
    "'FR' : \"France\",\n",
    "'IT' : \"Italy\",\n",
    "'AT' : \"Austria\",\n",
    "'ES' : \"Spain\",\n",
    "'PT' : \"Portugal\",\n",
    "'NO' : \"Norway\",\n",
    "'SE' : \"Sweden\",\n",
    "'FI' : \"Finland\",\n",
    "'SK' : \"Slovakia\",\n",
    "'RO' : \"Romania\",\n",
    "'BG' : \"Bulgaria\",\n",
    "'GR' : \"Greece\",\n",
    "'AL' : \"Albania\",\n",
    "'ME' : \"Montenegro\",\n",
    "'BA' : \"Bosnia and Herzegovina\",\n",
    "'HR' : \"Croatia\",\n",
    "'SI' : \"Slovenia\",\n",
    "'HU' : \"Hungary\",\n",
    "'RS' : \"Serbia\",\n",
    "'MK' : \"North Macedonia\",\n",
    "'XK' : \"Kosovo\",\n",
    "'PL' : \"Poland\",\n",
    "'UA' : \"Ukraine\",\n",
    "'BY' : \"Belarus\",\n",
    "'DE' : \"Germany\",\n",
    "'NL' : \"Netherlands\",\n",
    "'BE' : \"Belgium\",\n",
    "'LU' : \"Luxembourg\",\n",
    "'IE' : \"Ireland\",\n",
    "'UK' : \"United Kingdom\",\n",
    "'DK' : \"Denmark\",\n",
    "'EE' : \"Estonia\",\n",
    "'LV' : \"Latvia\",\n",
    "'LT' : \"Lithuania\",\n",
    "'CZ' : \"Czech Republic\"}\n",
    "\n",
    "\n",
    "JRC_hydropower_data[\"country\"] = JRC_hydropower_data.country_code.map(country_code_name_dict)\n",
    "\n",
    "JRC_hydropower_data = JRC_hydropower_data[[\"id\", \"name\", \"installed_capacity_MW\", \"type\", \"lat\", \"lon\", \"dam_height_m\", \"res_vol_km3\", \"country\"]]\n",
    "JRC_hydropower_data.rename(columns={\"id\": \"plant_source_id\", \"name\": \"name\", \"installed_capacity_MW\": \"capacity_mw\", \n",
    "                                    \"type\" : \"plant_type\", \"lat\" : \"plant_lat\", \"lon\" : \"plant_lon\"}, inplace=True)\n",
    "\n",
    "JRC_hydropower_data[\"plant_source\"] = \"JRC\"\n",
    "JRC_hydropower_data[\"res_attr_source\"] = np.where(pd.isna(JRC_hydropower_data[\"res_vol_km3\"]), np.nan, \"JRC\")\n",
    "JRC_hydropower_data[\"dam_height_source\"] = np.where(pd.isna(JRC_hydropower_data[\"dam_height_m\"]), np.nan, \"JRC\")\n",
    "\n",
    "\n",
    "def change_type(row):\n",
    "    if row['plant_type'] == 'HDAM':\n",
    "        return 'STO'\n",
    "    elif row['plant_type'] =='HROR':\n",
    "        return 'ROR'\n",
    "    elif row['plant_type'] == \"HPHS\": \n",
    "        return \"PS\"\n",
    "    \n",
    "JRC_hydropower_data[\"plant_type\"] = JRC_hydropower_data.apply(change_type, axis=1)\n",
    "JRC_hydropower_data[\"plant_type_source\"] = np.where(pd.isna(JRC_hydropower_data[\"plant_type\"]), np.nan, \"JRC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shah0012/.conda/envs/python_env/lib/python3.7/site-packages/pyproj/crs/crs.py:131: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "# US hydropower data\n",
    "# EHA provide hydropower installed capacity and head data.\n",
    "# HILARRI provides hydropower link to grand data\n",
    "# Both EHA and HILARRI have eha_ptid which is common between them. Therefore we can link the data using eha_ptid\n",
    "\n",
    "EHA_unit_level_hydropower_data = load_input_data(\"/home/shah0012/GloHydroRes/Input_data/USA_ORNL_EHAHydroUnit_FY2023\", \"shp\", \"Unit_external2023.shp\")\n",
    "USA_HILARRI_data = load_input_data(\"/home/shah0012/GloHydroRes/Input_data/HILARRI_v1_1_Shapefile\", \"shp\", \"HILARRI_v1_1_Public.shp\")\n",
    "EHA_unit_level_hydropower_data[\"head_m\"] = EHA_unit_level_hydropower_data.Head_ft * 0.3048 # Convert head from ft to m\n",
    "USA_HILARRI_data.grand_id =  np.where(USA_HILARRI_data.grand_id == \"NA\", np.nan, USA_HILARRI_data.grand_id)\n",
    "USA_HILARRI_data.grand_id = USA_HILARRI_data.grand_id.astype(float)\n",
    "\n",
    "\n",
    "# EHA provides data at unit level. We need to aggregate the data at plant level. Here sum the capacity, take the mean of the head. \n",
    "# Operation year is taken as the maximum of the operation year of the units\n",
    "EHA_plant_level_hydropower_data = EHA_unit_level_hydropower_data.groupby([\"EHA_PtID\"]).MW.sum().reset_index()\n",
    "EHA_plant_level_hydropower_data[\"head_m\"] = EHA_unit_level_hydropower_data.groupby([\"EHA_PtID\"]).head_m.mean().reset_index()[\"head_m\"]\n",
    "EHA_plant_level_hydropower_data[\"year\"] = EHA_unit_level_hydropower_data.groupby([\"EHA_PtID\"]).OpYear.max().reset_index()[\"OpYear\"]\n",
    "EHA_plant_level_hydropower_data[\"head_source\"] = \"ORNL\" # ORNL is for Oak Ridge National Laboratory which is the source of both EHA and HILARRI data\n",
    "\n",
    "\n",
    "eha_ptid_pt_name_dict = dict(zip(USA_HILARRI_data.eha_ptid, USA_HILARRI_data.pt_name)) # As EHA provides turbine name therefore plant name is taken from HILARRI data\n",
    "eha_ptid_longitude_dict = dict(zip(USA_HILARRI_data.eha_ptid, USA_HILARRI_data.pt_lon)) # As EHA provides turbine longitude therefore plant longitude is taken from HILARRI data\n",
    "eha_ptid_latitude_dict = dict(zip(USA_HILARRI_data.eha_ptid, USA_HILARRI_data.pt_lat)) # As EHA provides turbine latitude therefore plant latitude is taken from HILARRI data\n",
    "eha_ptid_grand_id_dict = dict(zip(USA_HILARRI_data.eha_ptid, USA_HILARRI_data.grand_id)) # eha_ptid is linked to grand_id in HILARRI data\n",
    "\n",
    "\n",
    "EHA_plant_level_hydropower_data[\"plant_name\"] = EHA_plant_level_hydropower_data.EHA_PtID.map(eha_ptid_pt_name_dict)\n",
    "EHA_plant_level_hydropower_data[\"plant_lat\"] = EHA_plant_level_hydropower_data.EHA_PtID.map(eha_ptid_latitude_dict)\n",
    "EHA_plant_level_hydropower_data[\"plant_lon\"] = EHA_plant_level_hydropower_data.EHA_PtID.map(eha_ptid_longitude_dict)\n",
    "EHA_plant_level_hydropower_data[\"res_dam_source_id\"] = EHA_plant_level_hydropower_data.EHA_PtID.map(eha_ptid_grand_id_dict)\n",
    "EHA_plant_level_hydropower_data[\"res_dam_source\"] = np.where(EHA_plant_level_hydropower_data.res_dam_source_id.isna(), np.nan, \"GranD\")\n",
    "\n",
    "EHA_plant_level_hydropower_data = EHA_plant_level_hydropower_data[~pd.isna(EHA_plant_level_hydropower_data.plant_name)] # Remove the plants which do not have name\n",
    "\n",
    "\n",
    "# Convert lat and lon to float\n",
    "EHA_plant_level_hydropower_data.plant_lat = np.where(EHA_plant_level_hydropower_data.plant_lat == \"NA\", np.nan, EHA_plant_level_hydropower_data.plant_lat)\n",
    "EHA_plant_level_hydropower_data.plant_lon = np.where(EHA_plant_level_hydropower_data.plant_lon == \"NA\", np.nan, EHA_plant_level_hydropower_data.plant_lon)\n",
    "EHA_plant_level_hydropower_data.plant_lat = EHA_plant_level_hydropower_data.plant_lat.astype(float)\n",
    "EHA_plant_level_hydropower_data.plant_lon = EHA_plant_level_hydropower_data.plant_lon.astype(float)\n",
    "\n",
    "\n",
    "EHA_plant_level_hydropower_data[\"country\"] = \"United States of America\"\n",
    "\n",
    "\n",
    "# Convert the data to geodataframe\n",
    "geometry = [Point(xy) for xy in zip(EHA_plant_level_hydropower_data.plant_lon, EHA_plant_level_hydropower_data.plant_lat)]\n",
    "crs = {'init': 'epsg:4326'}\n",
    "EHA_plant_level_hydropower_data = gpd.GeoDataFrame(EHA_plant_level_hydropower_data, crs=crs, geometry=geometry)\n",
    "\n",
    "EHA_plant_level_hydropower_data = EHA_plant_level_hydropower_data[['EHA_PtID', 'MW', 'head_m', 'head_source', 'plant_name', 'plant_lat', 'plant_lon', 'year', 'res_dam_source_id', 'res_dam_source']]\n",
    "\n",
    "EHA_plant_level_hydropower_data.rename(columns={\"MW\": \"capacity_mw\", \"EHA_PtID\" : \"plant_source_id\", \"plant_name\" : \"name\"}, inplace=True)\n",
    "\n",
    "\n",
    "EHA_plant_level_hydropower_data[\"plant_source\"] = \"EHA\"\n",
    "\n",
    "## As EHA data already contain grand id, we can directly get reservoir data from GranD data\n",
    "\n",
    "# GranD dam data\n",
    "grand_data = load_input_data(\"/home/shah0012/GloHydroRes/Input_data/GRanD_Version_1_3\", \"shp\", \"GRanD_dams_v1_3.shp\")\n",
    "\n",
    "EHA_plant_level_hydropower_data.res_dam_source_id.dropna().duplicated().sum()\n",
    "\n",
    "EHA_grand_ids = EHA_plant_level_hydropower_data.res_dam_source_id.dropna()\n",
    "\n",
    "# Here two plants can have the same grand id. Therefore we need to take care of duplicates.\n",
    "# Here isin function with work because we are making first time entries in EHA plant level data. Therefore we do not need to worry about duplicates\n",
    "# However if we are updating the data then we need to take care of duplicates\n",
    "filtered_grand_data = grand_data[grand_data.GRAND_ID.isin(EHA_grand_ids)]\n",
    "filtered_grand_data = filtered_grand_data[['GRAND_ID', 'RES_NAME', 'DAM_NAME', 'RIVER', 'DAM_HGT_M', 'AREA_SKM', 'CAP_MCM', \"DEPTH_M\"]]\n",
    "filtered_grand_data[\"res_vol_km3\"] = filtered_grand_data[\"CAP_MCM\"]*0.001 # Convert volume from Mm3 to km3\n",
    "filtered_grand_data.drop(columns=[\"CAP_MCM\"], inplace=True)\n",
    "filtered_grand_data.rename(columns={\"RES_NAME\": \"res_name\", \"DAM_NAME\": \"dam_name\", \"RIVER\": \"river\", \"DAM_HGT_M\": \"dam_height_m\", \"AREA_SKM\": \"res_area_km2\", \"DEPTH_M\" : \"res_avg_depth_m\"}, inplace=True)\n",
    "filtered_grand_data[\"res_attr_id\"] = filtered_grand_data.GRAND_ID\n",
    "filtered_grand_data[\"res_attr_source\"] = \"GranD\"\n",
    "filtered_grand_data[\"dam_height_source\"] = \"GranD\"\n",
    "\n",
    "EHA_plant_level_hydropower_data = pd.merge(EHA_plant_level_hydropower_data, filtered_grand_data, left_on='res_dam_source_id', right_on='GRAND_ID', how='left')\n",
    "EHA_plant_level_hydropower_data.drop(columns=[\"GRAND_ID\"], inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RePP data\n",
    "Africa_hydropower_data = pd.read_excel(\"/home/shah0012/GloHydroRes/Input_data/Rebecca_peters_Africa_data/RePP_Petersetal.xlsx\", sheet_name= \"S3 HPPD\")\n",
    "Africa_hydropower_data = gpd.GeoDataFrame(Africa_hydropower_data, geometry=gpd.points_from_xy(Africa_hydropower_data.lon, Africa_hydropower_data.lat), crs=\"EPSG:4326\")\n",
    "Africa_hydropower_data = Africa_hydropower_data[Africa_hydropower_data.stat_inf == \"E\"] # To get only the existing hydropower plants\n",
    "Africa_hydropower_data = Africa_hydropower_data[[\"HPPD_ID\", \"lon\", \"lat\", \"country\", \"HPP_name\", \"type\", \"main_river\", \"g_hgt_m\", \"first_o_start\", \"g_cap_mw\"]]\n",
    "Africa_hydropower_data.rename(columns={\"HPPD_ID\": \"plant_source_id\", \"HPP_name\": \"name\", \"g_hgt_m\": \"dam_height_m\", \n",
    "                                       \"first_o_start\": \"year\", \"main_river\" : \"river\", \"type\" : \"plant_type\",\n",
    "                                       \"g_cap_mw\" : \"capacity_mw\", \"lon\" : \"plant_lon\", \"lat\" : \"plant_lat\"}, inplace=True)\n",
    "Africa_hydropower_data[\"dam_height_source\"] = np.where(pd.isna(Africa_hydropower_data.dam_height_m), np.nan, \"RePP\")\n",
    "Africa_hydropower_data[\"plant_source\"] = \"RePP\"\n",
    "\n",
    "def change_type(row):\n",
    "    if row['plant_type'] == 'Reservoir':\n",
    "        return 'STO'\n",
    "    elif row['plant_type'] =='RoR':\n",
    "        return 'ROR'\n",
    "    elif row['plant_type'] == \"Pumped Storage\": \n",
    "        return \"PS\"\n",
    "    elif row['plant_type'] == \"RoR \":\n",
    "        return \"ROR\"\n",
    "    \n",
    "Africa_hydropower_data[\"plant_type\"] = Africa_hydropower_data.apply(change_type, axis=1)\n",
    "Africa_hydropower_data[\"plant_type_source\"] = \"RePP\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"country\", \"name\", \"capacity_mw\", \"plant_lat\", \"plant_lon\", \"plant_type\", \"plant_type_source\", \"year\",  \n",
    "           \"plant_source\", \"plant_source_id\", \"dam_name\", \"dam_height_m\", \"dam_height_source\", \"res_name\", \"res_dam_source\", \"res_dam_source_id\",\n",
    "             \"man_dam_lat\", \"man_dam_lon\", \"river\", \"head_m\", \"head_source\", \"res_avg_depth_m\", \"res_area_km2\", \"res_vol_km3\",\n",
    "             \"res_attr_source\", \"res_attr_id\", \"hydrolakes_id\", \"final_comments\"] \n",
    "\n",
    "glohydrores =  pd.DataFrame(columns=columns)\n",
    "glohydrores = pd.concat([glohydrores, WRI_hydro, JRC_hydropower_data, EHA_plant_level_hydropower_data, Africa_hydropower_data], axis=0)\n",
    "glohydrores = gpd.GeoDataFrame(glohydrores, geometry=gpd.points_from_xy(glohydrores.plant_lon, glohydrores.plant_lat), crs=\"EPSG:4326\")\n",
    "glohydrores.set_index(np.arange(glohydrores.shape[0]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each source of hydropower data has a unique plant_source_id. However, the unique IDs between the two datasets do not match, so we can use the plant_source_id for further analysis.\n",
    "#Note: If two sources have similar IDs, such as those that begin number of sequence, this approach may not be suitable.\n",
    "glohydrores.plant_source_id.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEM data\n",
    "DEM_data = load_input_data(\"/home/shah0012/GloHydroRes/Input_data/DEM\", \"tif\", \"hyd_glo_dem_15s.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_values(row, other_gdf,  dam_lat_column = \"LAT_DD\", dam_lon_column = \"LONG_DD\",   value_column = 'geometry', unique_id = 'GRAND_ID'):\n",
    "    \"\"\" Get nearest 3 dams for which distance between the dam and the hydro power plant is less than 10 km\n",
    "    \n",
    "    : param row: pd.Series: row of the GeoDataFrame containing the hydro power plant information\n",
    "    : param other_gdf: gpd.GeoDataFrame: GeoDataFrame of the other dams\n",
    "    : param dam_lat_column: str: column name of the latitude in dam dataset\n",
    "    : param dam_lon_column: str: column name of the longitude in dam dataset\n",
    "    : param value_column: str: column name of the geometry in the dam dataset\n",
    "    : param unique_id: str: column name of the unique id in the dam dataset.\n",
    "    : return nearest_id_list: list: list of the nearest dam ids (unique ids)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create an union of the other GeoDataFrame's geometeries\n",
    "    nearest_id_list = []\n",
    "    other_points = other_gdf[value_column].unary_union\n",
    "    i = -1\n",
    "    while i < 3: \n",
    "        \n",
    "        nearest_geoms = nearest_points(row.geometry, other_points)\n",
    "        idx = other_gdf[other_gdf[value_column] == nearest_geoms[1]].index\n",
    "        idx = idx[0]\n",
    "        sel_lat_val = row.plant_lat\n",
    "        sel_lon_val = row.plant_lon\n",
    "        nearest_ids = other_gdf.at[idx, unique_id]\n",
    "        dam_lat =  other_gdf[dam_lat_column].to_numpy()[other_gdf[unique_id].to_numpy() == nearest_ids].item()\n",
    "        dam_lon = other_gdf[dam_lon_column].to_numpy()[other_gdf[unique_id].to_numpy() == nearest_ids].item()        \n",
    "        p1 = (dam_lat, dam_lon)\n",
    "        p2 = (sel_lat_val, sel_lon_val)\n",
    "        distance = geodesic(p1, p2).km\n",
    "\n",
    "        if distance > 10:\n",
    "            break\n",
    "        else:\n",
    "             nearest_id_list.append(nearest_ids)\n",
    "             i += 1\n",
    "        other_gdf = other_gdf.query(f'{unique_id} != {nearest_ids}')\n",
    "        other_points = other_gdf[value_column].unary_union\n",
    "\n",
    "    return nearest_id_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine GloHydroRes with GRanD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grand_manual_hydropower_reservoir_identification_func(dam_dataset : gpd.GeoDataFrame , hydropower_data, dem_data : xr.Dataset , unique_id  = 'GRAND_ID' ) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\" Identify the reservoir for the hydro power plants in the glohydrores dataset using the GranD dataset. Final dataframe will contain the information about plant id and \n",
    "        the identified reservoir id\n",
    "\n",
    "    : param dam_dataset: gpd.GeoDataFrame: GeoDataFrame of the GranD dataset\n",
    "    : param hydropower_data: pd.DataFrame: DataFrame of the hydro power plants\n",
    "    : param dem_data: xr.Dataset: xarray Dataset of the DEM data\n",
    "    : param unique_id: str: column name of the unique id in the dam dataset.\n",
    "    : return df: pd.DataFrame: DataFrame of the identified reservoirs\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #grand_res_id = dam_dataset[unique_id].to_list()\n",
    "    plant_id_list = []\n",
    "    matched_grand_id_list = []\n",
    "\n",
    "    unmatched_hydropower_data = hydropower_data[hydropower_data['res_dam_source_id'].isna()]\n",
    "    \n",
    "\n",
    "    n = -1       \n",
    "    for row in unmatched_hydropower_data.itertuples(index=False):\n",
    "            n += 1\n",
    "            print(n)\n",
    "            plant_name = row.name\n",
    "            plant_id = row.plant_source_id\n",
    "            plant_lat = row.plant_lat\n",
    "            plant_lon = row.plant_lon\n",
    "            \n",
    "            # Get hydropower plant location elevation from DEM data\n",
    "            clipped_data = dem_data.sel(\n",
    "                        y = plant_lat,\n",
    "                        x = plant_lon,\n",
    "                        method = \"nearest\")\n",
    "        \n",
    "            plant_dem = float(clipped_data.values) \n",
    "\n",
    "            # Get three nearest dams from the GranD dataset (distance between the dam and the hydro power plant is less than 10 km)                \n",
    "            nearest_grand_ids = get_nearest_values(row, dam_dataset)\n",
    "\n",
    "            ## BELOW TO CHECK IF NONE OF THE RESERVIOR IS LOCATED CLOSE TO 10KM\n",
    "\n",
    "            if nearest_grand_ids:\n",
    "                for id in nearest_grand_ids:\n",
    "                    sel_dam_column = dam_dataset.query(f'{unique_id} == @id')\n",
    "                    dam_name = str(sel_dam_column.DAM_NAME.iloc[0])\n",
    "                    alt_name = str(sel_dam_column.ALT_NAME.iloc[0])\n",
    "                    res_name = str(sel_dam_column.RES_NAME.iloc[0])\n",
    "                    dam_lat = sel_dam_column.LAT_DD.iloc[0]\n",
    "                    dam_lon = sel_dam_column.LONG_DD.iloc[0]\n",
    "                    clipped_data = dem_data.sel(\n",
    "                                y = dam_lat,\n",
    "                                x = dam_lon, method = \"nearest\")\n",
    "                    \n",
    "                    # Get dam location elevation from DEM data\n",
    "                    dam_dem = float(clipped_data.values)\n",
    "                \n",
    "                    # As grand provides detailed information about the resevoir, we are using dam name, alternative name and reservoir name to match the power plant with the dam\n",
    "                    # Further if name does not match, we are using the elevation of the power plant and the dam to match the power plant with the dam\n",
    "                    if (bool(set(plant_name.lower().split()).intersection(dam_name.lower().split())) or \n",
    "                        bool(set(plant_name.lower().split()).intersection(alt_name.lower().split())) or \n",
    "                        bool(set(plant_name.lower().split()).intersection(res_name.lower().split())) or \n",
    "                        plant_dem <= dam_dem):\n",
    "                        matched_grand_id_list.append(id)\n",
    "                        plant_id_list.append(plant_id)\n",
    "                        break   \n",
    "    df = pd.DataFrame({\n",
    "        \"plant_source_id\" : plant_id_list,\n",
    "        \"res_dam_source_id\" : matched_grand_id_list})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-39835b0ab8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplant_id_matched_grand_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrand_manual_hydropower_reservoir_identification_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdam_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrand_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhydropower_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglohydrores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdem_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEM_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-f4e661c07300>\u001b[0m in \u001b[0;36mgrand_manual_hydropower_reservoir_identification_func\u001b[0;34m(dam_dataset, hydropower_data, dem_data, unique_id)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Get three nearest dams from the GranD dataset (distance between the dam and the hydro power plant is less than 10 km)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mnearest_grand_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nearest_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdam_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m## BELOW TO CHECK IF NONE OF THE RESERVIOR IS LOCATED CLOSE TO 10KM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1e07e681a0e4>\u001b[0m in \u001b[0;36mget_nearest_values\u001b[0;34m(row, other_gdf, dam_lat_column, dam_lon_column, value_column, unique_id)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnearest_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnearest_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_gdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_gdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnearest_geoms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msel_lat_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplant_lat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5501\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5502\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    268\u001b[0m     ):\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# Call the method on lvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/geopandas/array.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_binop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/geopandas/array.py\u001b[0m in \u001b[0;36m_binop\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;31m# If the operator is not defined for the underlying objects,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;31m# a TypeError should be raised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/geopandas/array.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;31m# If the operator is not defined for the underlying objects,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;31m# a TypeError should be raised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/shapely/geometry/base.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    279\u001b[0m         return (\n\u001b[1;32m    280\u001b[0m             \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/shapely/speedups/_speedups.pyx\u001b[0m in \u001b[0;36mcoordseq_iter\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/shapely/coords.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cseq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/shapely/coords.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__p__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/shapely/geometry/base.py\u001b[0m in \u001b[0;36mis_empty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;34m\"\"\"True if the set of points in this geometry is empty, else False\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geom\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_empty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/shapely/predicates.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, this)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.7/site-packages/shapely/geos.py\u001b[0m in \u001b[0;36merrcheck_predicate\u001b[0;34m(result, func, argtuple)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck_predicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;34m\"\"\"Result is 2 on exception, 1 on True, 0 on False\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plant_id_matched_grand_id = grand_manual_hydropower_reservoir_identification_func(dam_dataset=grand_data, hydropower_data=glohydrores, dem_data=DEM_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First insert update data in the plant id and matched grand id dataframe\n",
    "# Then update the glohydrores dataframe with updated dataframe\n",
    "\n",
    "plant_id_matched_grand_id[\"dam_name\"] = plant_id_matched_grand_id.res_dam_source_id.map(dict(zip(grand_data.GRAND_ID, grand_data.DAM_NAME)))\n",
    "plant_id_matched_grand_id[\"dam_height_m\"] = plant_id_matched_grand_id.res_dam_source_id.map(dict(zip(grand_data.GRAND_ID, grand_data.DAM_HGT_M)))\n",
    "plant_id_matched_grand_id[\"dam_height_source\"] = \"GranD\"\n",
    "plant_id_matched_grand_id[\"res_name\"] = plant_id_matched_grand_id.res_dam_source_id.map(dict(zip(grand_data.GRAND_ID, grand_data.RES_NAME)))\n",
    "plant_id_matched_grand_id[\"res_dam_source\"] = \"GranD\"\n",
    "plant_id_matched_grand_id[\"river\"] = plant_id_matched_grand_id.res_dam_source_id.map(dict(zip(grand_data.GRAND_ID, grand_data.RIVER)))\n",
    "plant_id_matched_grand_id[\"res_avg_depth_m\"] = plant_id_matched_grand_id.res_dam_source_id.map(dict(zip(grand_data.GRAND_ID, grand_data.DEPTH_M)))\n",
    "plant_id_matched_grand_id[\"res_area_km2\"] = plant_id_matched_grand_id.res_dam_source_id.map(dict(zip(grand_data.GRAND_ID, grand_data.AREA_SKM)))\n",
    "plant_id_matched_grand_id[\"res_vol_km3\"] = plant_id_matched_grand_id.res_dam_source_id.map(dict(zip(grand_data.GRAND_ID, grand_data.CAP_MCM)))\n",
    "plant_id_matched_grand_id[\"res_vol_km3\"] = plant_id_matched_grand_id[\"res_vol_km3\"]*0.001 # Convert volume from Mm3 to km3\n",
    "plant_id_matched_grand_id[\"res_attr_source\"] = \"GranD\"\n",
    "plant_id_matched_grand_id[\"res_attr_id\"] = plant_id_matched_grand_id.res_dam_source_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shah0012/.conda/envs/python_env/lib/python3.7/site-packages/geopandas/geodataframe.py:1351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First insert the matched grand id in the glohydrores dataset res_dam_id column\n",
    "glohydrores[\"dam_name\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.dam_name)))\n",
    "glohydrores[\"dam_height_m\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.dam_height_m)))\n",
    "glohydrores[\"dam_height_source\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.dam_height_source)))\n",
    "glohydrores[\"res_name\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.res_name)))\n",
    "glohydrores[\"res_dam_source\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.res_dam_source)))\n",
    "glohydrores[\"res_dam_source_id\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.res_dam_source_id)))\n",
    "glohydrores[\"river\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.river)))\n",
    "glohydrores[\"res_avg_depth_m\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.res_avg_depth_m)))\n",
    "glohydrores[\"res_area_km2\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.res_area_km2)))\n",
    "glohydrores[\"res_vol_km3\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.res_vol_km3)))\n",
    "glohydrores[\"res_attr_source\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.res_attr_source)))\n",
    "glohydrores[\"res_attr_id\"] =  glohydrores.plant_source_id.map(dict(zip(plant_id_matched_grand_id.plant_source_id, plant_id_matched_grand_id.res_attr_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine GloHydroRes with GeoDAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GeoDAR data\n",
    "geodar_dam_data = load_input_data(\"/home/shah0012/GloHydroRes/Input_data/GeoDAR_v10_v11/\", \"shp\", \"GeoDAR_v11_dams.shp\")\n",
    "geodar_dam_data = geodar_dam_data.to_crs('epsg:4326')\n",
    "\n",
    "gdat_res_data = load_input_data(\"/home/shah0012/GloHydroRes/Input_data/GDAT/GDAT_data_v1/data/\", \"shp\", \"GDAT_v1_catchments.shp\")\n",
    "gdat_res_data = gdat_res_data.to_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdat_data_func(n, dam_data):\n",
    "    \"\"\" Get the GDAT data attributes for matched GeoDAR/HydroLakes reservoirs. GDAT reservoir is considered as matched \n",
    "    if the GeoDAR/HydroLakes dam is located within the GDAT reservoir  \n",
    "\n",
    "    : param n: int: index of selected GeoDAR dam\n",
    "    : param dam_data: gpd.GeoDataFrame: dam dataset of GeoDAR\n",
    "    : return dam_name: str: name of the dam\n",
    "    : return res_name: str: name of the reservoir\n",
    "    : return river_name: str: name of the river\n",
    "    : return dam_hgt: float: height of the dam\n",
    "    : return dam_hgt_source: str: source of the dam height\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Here w\n",
    "    if gdat_res_data[gdat_res_data.geometry.contains(dam_data.geometry[n])].empty:\n",
    "        dam_name = None\n",
    "        dam_height_m = None\n",
    "        dam_height_source = None\n",
    "        res_name = None\n",
    "        river = None\n",
    "        res_avg_depth_m = None\n",
    "        res_area_km2 = None\n",
    "        res_vol_km3 = None\n",
    "        res_attr_source = None\n",
    "        res_attr_id = None\n",
    "    else: # Only return if there is only one matching\n",
    "        sel_gdat_data = gdat_res_data[gdat_res_data.geometry.contains(dam_data.geometry[n])]\n",
    "        if sel_gdat_data.shape[0] == 1: ## THIS IF ONLY ONE MATCHING  \n",
    "            dam_name = sel_gdat_data.Dam_Name.iloc[0]\n",
    "            dam_height_m = sel_gdat_data.Height.iloc[0]\n",
    "            dam_height_source = \"GDAT\"\n",
    "            res_name = sel_gdat_data.Reservoir.iloc[0]\n",
    "            river = sel_gdat_data.River.iloc[0]\n",
    "            res_avg_depth_m = sel_gdat_data.Avg_Depth.iloc[0]\n",
    "            res_area_km2 = sel_gdat_data.Area_Con.iloc[0]\n",
    "            res_vol_km3 = sel_gdat_data.Volume_Con.iloc[0]*0.001\n",
    "            res_attr_source = \"GDAT\"\n",
    "            res_attr_id = sel_gdat_data.Feature_ID.iloc[0]\n",
    "\n",
    "\n",
    "        else: ## THIS IF MULTIPLE MATCHING\n",
    "            dam_name = None\n",
    "            dam_height_m = None\n",
    "            dam_height_source = None\n",
    "            res_name = None\n",
    "            river = None\n",
    "            res_avg_depth_m = None\n",
    "            res_area_km2 = None\n",
    "            res_vol_km3 = None\n",
    "            res_attr_source = None\n",
    "            res_attr_id = None\n",
    "\n",
    "    return dam_name, dam_height_m, dam_height_source,  res_name, river, res_avg_depth_m, res_area_km2, res_vol_km3, res_attr_source, res_attr_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geodar_manual_hydropower_reservoir_identification_func(dam_dataset : gpd.GeoDataFrame, hydropower_data, dem_data : xr.Dataset, unique_id = 'id_v11') -> pd.DataFrame:\n",
    "    \"\"\" Identify the reservoir for the hydro power plants in the glohydrores dataset using the GeoDAR/HydroLakes dataset. Final dataframe will contain the information about plant id, \n",
    "        the identified reservoir id, reservoir and dam attributes from the GDAT dataset\n",
    "\n",
    "\n",
    "    : param dam_dataset: gpd.GeoDataFrame: GeoDataFrame of the GeoDAR/HydroLakes dataset\n",
    "    : param hydropower_data: pd.DataFrame: DataFrame of the hydro power plants\n",
    "    : param dem_data: xr.Dataset: xarray Dataset of the DEM data\n",
    "    : param unique_id: str: column name of the unique id in the dam dataset.\n",
    "    : return df: pd.DataFrame: DataFrame of the identified reservoirs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plant_id_list = []\n",
    "    matched_id_list = []\n",
    "    dam_name_list = []\n",
    "    dam_height_list = []\n",
    "    dam_height_source_list = []\n",
    "    res_name_list = []\n",
    "    river_list = []\n",
    "    res_avg_depth_list = []\n",
    "    res_area_list = []\n",
    "    res_vol_list = []\n",
    "    res_attr_source_list = []\n",
    "    res_attr_id_list = []\n",
    "    \n",
    "    unmatched_hydropower_data = hydropower_data[hydropower_data['res_dam_source_id'].isna()]\n",
    "\n",
    "\n",
    "    n = -1\n",
    "    for row in unmatched_hydropower_data.itertuples(index=False):\n",
    "        n += 1\n",
    "        print(n)\n",
    "        plant_id = row.plant_source_id\n",
    "        sel_lat_val = row.plant_lat\n",
    "        sel_lon_val = row.plant_lon\n",
    "        clipped_data = dem_data.sel(\n",
    "                        y = sel_lat_val,\n",
    "                        x = sel_lon_val,\n",
    "                        method = \"nearest\")\n",
    "        \n",
    "        plant_dem = float(clipped_data.values) \n",
    "        nearest_geodar_ids = get_nearest_values(row = row, other_gdf = dam_dataset,  dam_lat_column = \"lat\", dam_lon_column= \"lon\", unique_id=\"id_v11\")\n",
    "\n",
    "        if nearest_geodar_ids:\n",
    "             for dam_id in nearest_geodar_ids:\n",
    "                    sel_dam_column = dam_dataset.query(f'{unique_id} == @dam_id')\n",
    "                    idx = sel_dam_column.index[0]\n",
    "                    print(sel_dam_column.geometry)\n",
    "                    dam_lat = sel_dam_column.lat.iloc[0]\n",
    "                    dam_lon = sel_dam_column.lon.iloc[0]      \n",
    "                    clipped_data = dem_data.sel(\n",
    "                                y = dam_lat,\n",
    "                                x = dam_lon, method = \"nearest\")\n",
    "                    dam_dem = float(clipped_data.values)\n",
    "                    if plant_dem <= dam_dem:\n",
    "                         dam_name, dam_height_m, dam_height_source,  res_name, river, res_avg_depth_m, res_area_km2, res_vol_km3, res_attr_source, res_attr_id = get_gdat_data_func(idx, dam_data=geodar_dam_data)\n",
    "                         plant_id_list.append(plant_id)\n",
    "                         matched_id_list.append(dam_id)\n",
    "                         dam_name_list.append(dam_name)\n",
    "                         dam_height_list.append(dam_height_m)\n",
    "                         dam_height_source_list.append(dam_height_source)\n",
    "                         res_name_list.append(res_name)\n",
    "                         river_list.append(river)\n",
    "                         res_avg_depth_list.append(res_avg_depth_m)\n",
    "                         res_area_list.append(res_area_km2)\n",
    "                         res_vol_list.append(res_vol_km3)\n",
    "                         res_attr_source_list.append(res_attr_source)\n",
    "                         res_attr_id_list.append(res_attr_id)\n",
    "                         break\n",
    "\n",
    "     \n",
    "    geodar_df = pd.DataFrame({\n",
    "        \"plant_source_id\" : plant_id_list,\n",
    "        \"res_dam_source_id\" : matched_id_list,\n",
    "        \"dam_name\" : dam_name_list,\n",
    "        \"dam_height_m\" : dam_height_list,\n",
    "        \"dam_height_source\" : dam_height_source_list,\n",
    "        \"res_name\" : res_name_list,\n",
    "        \"river\" : river_list,\n",
    "        \"res_avg_depth_m\" : res_avg_depth_list,\n",
    "        \"res_area_km2\" : res_area_list,\n",
    "        \"res_vol_km3\" : res_vol_list,\n",
    "        \"res_attr_source\" : res_attr_source_list,\n",
    "        \"res_attr_id\" : res_attr_id_list})\n",
    "    \n",
    "    return geodar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "127    POINT (19.86140 41.30800)\n",
      "Name: geometry, dtype: geometry\n",
      "82    POINT (19.90831 41.40153)\n",
      "Name: geometry, dtype: geometry\n",
      "4\n",
      "112    POINT (19.89809 41.68145)\n",
      "Name: geometry, dtype: geometry\n",
      "28    POINT (19.83266 41.69192)\n",
      "Name: geometry, dtype: geometry\n",
      "5\n",
      "23281    POINT (19.62749 42.02010)\n",
      "Name: geometry, dtype: geometry\n",
      "6\n",
      "185    POINT (13.73193 -12.47061)\n",
      "Name: geometry, dtype: geometry\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "227    POINT (-69.06667 -30.18333)\n",
      "Name: geometry, dtype: geometry\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "225    POINT (-65.25145 -24.45026)\n",
      "Name: geometry, dtype: geometry\n",
      "16\n",
      "17\n",
      "220    POINT (-64.36183 -27.65094)\n",
      "Name: geometry, dtype: geometry\n"
     ]
    }
   ],
   "source": [
    "plant_id_matched_geodar_id = geodar_manual_hydropower_reservoir_identification_func(dam_dataset=geodar_dam_data, hydropower_data=glohydrores, dem_data=DEM_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_id_matched_geodar_id[\"res_dam_source\"] = \"GeoDAR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the glohydrores dataframe with the matched GeoDAR data\n",
    "\n",
    "row_indices = glohydrores.loc[glohydrores.plant_source_id.isin(plant_id_matched_geodar_id.plant_source_id)].index\n",
    "glohydrores.loc[row_indices, [\"dam_name\", \"dam_height_m\", \"dam_height_source\", \"res_name\", \"river\", \"res_avg_depth_m\", \"res_area_km2\", \"res_vol_km3\", \"res_attr_source\", \"res_attr_id\", \"res_dam_source_id\", \"res_dam_source\"]] = plant_id_matched_geodar_id[[\"dam_name\", \"dam_height_m\", \"dam_height_source\", \"res_name\", \"river\", \"res_avg_depth_m\", \"res_area_km2\", \"res_vol_km3\", \"res_attr_source\", \"res_attr_id\", \"res_dam_source_id\", \"res_dam_source\"]].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine GloHydroRes with HydroLakes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HydroLakes data\n",
    "hydrolakes_point_data = load_input_data(\"/home/shah0012/GloHydroRes/Input_data/HydroLakes/HydroLAKES_points_v10_shp/\", \"shp\", \"HydroLAKES_points_v10.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdat_data_hydrolakes_func(dam_data):\n",
    "    \"\"\" Get the GDAT data attributes for matched HydroLakes reservoirs. GDAT reservoir is considered as matched \n",
    "    if the HydroLakes dam is located within the GDAT reservoir  \n",
    "\n",
    "    : param dam_data: gpd.GeoDataFrame: dam dataset of HydroLakes\n",
    "    : return dam_name: str: name of the dam\n",
    "    : return res_name: str: name of the reservoir\n",
    "    : return river_name: str: name of the river\n",
    "    : return dam_hgt: float: height of the dam\n",
    "    : return dam_hgt_source: str: source of the dam height\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Here w\n",
    "    if gdat_res_data[gdat_res_data.geometry.contains(dam_data.geometry.iloc[0])].empty:\n",
    "        dam_name = None\n",
    "        dam_height_m = None\n",
    "        dam_height_source = None\n",
    "        res_name = None\n",
    "        river = None\n",
    "    else: # Only return if there is only one matching\n",
    "        sel_gdat_data = gdat_res_data[gdat_res_data.geometry.contains(dam_data.geometry.iloc[0])]\n",
    "        if sel_gdat_data.shape[0] == 1: ## THIS IF ONLY ONE MATCHING  \n",
    "            dam_name = sel_gdat_data.Dam_Name.iloc[0]\n",
    "            dam_height_m = sel_gdat_data.Height.iloc[0]\n",
    "            dam_height_source = \"GDAT\"\n",
    "            res_name = sel_gdat_data.Reservoir.iloc[0]\n",
    "            river = sel_gdat_data.River.iloc[0]\n",
    "\n",
    "        else: ## THIS IF MULTIPLE MATCHING\n",
    "            dam_name = None\n",
    "            dam_height_m = None\n",
    "            dam_height_source = None\n",
    "            res_name = None\n",
    "            river = None\n",
    "            \n",
    "    return dam_name, dam_height_m, dam_height_source, res_name, river\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hydrolakes_manual_hydropower_reservoir_identification_func(dam_dataset : gpd.GeoDataFrame, hydropower_data, dem_data : xr.Dataset, unique_id = 'Hylak_id') -> pd.DataFrame:\n",
    "    \"\"\" Identify the reservoir for the hydro power plants in the glohydrores dataset using the HydroLakes dataset. Final dataframe will contain the information about plant id and \n",
    "        the identified reservoir id\n",
    "\n",
    "\n",
    "    : param dam_dataset: gpd.GeoDataFrame: GeoDataFrame of the HydroLakes dataset\n",
    "    : param hydropower_data: pd.DataFrame: DataFrame of the hydro power plants\n",
    "    : param dem_data: xr.Dataset: xarray Dataset of the DEM data\n",
    "    : param unique_id: str: column name of the unique id in the dam dataset.\n",
    "    : return df: pd.DataFrame: DataFrame of the identified reservoirs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plant_id_list = []\n",
    "    matched_hydlak_id_list = []\n",
    "    dam_name_list = []\n",
    "    dam_height_list = []\n",
    "    dam_height_source_list = []\n",
    "    res_name_list = []\n",
    "    river_list = []\n",
    "    res_avg_depth_list = []\n",
    "    res_area_list = []\n",
    "    res_vol_list = []\n",
    "    res_attr_source_list = []\n",
    "    res_attr_id_list = []\n",
    "    \n",
    "    unmatched_hydropower_data = hydropower_data[hydropower_data['res_dam_source_id'].isna()]\n",
    "\n",
    "\n",
    "    n = -1\n",
    "    for row in unmatched_hydropower_data.itertuples(index=False):\n",
    "        n += 1\n",
    "        print(n)\n",
    "        plant_id = row.plant_source_id\n",
    "        sel_lat_val = row.plant_lat\n",
    "        sel_lon_val = row.plant_lon\n",
    "        clipped_data = dem_data.sel(\n",
    "                        y = sel_lat_val,\n",
    "                        x = sel_lon_val,\n",
    "                        method = \"nearest\")\n",
    "        \n",
    "        plant_dem = float(clipped_data.values) \n",
    "        nearest_hydrolakes_ids = get_nearest_values(row = row, other_gdf = dam_dataset,  dam_lat_column = \"Pour_lat\", dam_lon_column= \"Pour_long\", unique_id=\"Hylak_id\")\n",
    "        print(n)\n",
    "\n",
    "        if nearest_hydrolakes_ids:\n",
    "             for dam_id in nearest_hydrolakes_ids:\n",
    "                    sel_dam_column = dam_dataset.query(f'{unique_id} == @dam_id')\n",
    "                    dam_lat = sel_dam_column.Pour_lat.iloc[0]\n",
    "                    dam_lon = sel_dam_column.Pour_long.iloc[0]\n",
    "                    clipped_data = dem_data.sel(\n",
    "                                y = dam_lat,\n",
    "                                x = dam_lon, method = \"nearest\")\n",
    "                    dam_dem = float(clipped_data.values)\n",
    "\n",
    "                    if plant_dem <= dam_dem:\n",
    "                        res_avg_depth_m = sel_dam_column.Depth_avg.iloc[0]\n",
    "                        res_area_km2 = sel_dam_column.Lake_area.iloc[0]\n",
    "                        res_vol_km3 = sel_dam_column.Vol_total.iloc[0]\n",
    "                        res_attr_source = \"HydroLakes\"\n",
    "                        res_attr_id = sel_dam_column.Hylak_id.iloc[0]\n",
    "                        \n",
    "                        plant_id_list.append(plant_id)\n",
    "                        matched_hydlak_id_list.append(dam_id)\n",
    "                        res_avg_depth_list.append(res_avg_depth_m)\n",
    "                        res_area_list.append(res_area_km2)\n",
    "                        res_vol_list.append(res_vol_km3)\n",
    "                        res_attr_source_list.append(res_attr_source)\n",
    "                        res_attr_id_list.append(res_attr_id)\n",
    "                         \n",
    "                        dam_name, dam_height_m, dam_height_source, res_name, river = get_gdat_data_hydrolakes_func(dam_data=sel_dam_column)\n",
    "                        dam_name_list.append(dam_name)\n",
    "                        dam_height_list.append(dam_height_m)\n",
    "                        dam_height_source_list.append(dam_height_source)\n",
    "                        res_name_list.append(res_name)\n",
    "                        river_list.append(river)\n",
    "                        break\n",
    "     \n",
    "    hydroalakes_df = pd.DataFrame({\n",
    "        \"plant_source_id\" : plant_id_list,\n",
    "        \"res_dam_source_id\" : matched_hydlak_id_list,\n",
    "        \"dam_name\" : dam_name_list,\n",
    "        \"dam_height_m\" : dam_height_list,\n",
    "        \"dam_height_source\" : dam_height_source_list,\n",
    "        \"res_name\" : res_name_list,\n",
    "        \"river\" : river_list,\n",
    "        \"res_avg_depth_m\" : res_avg_depth_list,\n",
    "        \"res_area_km2\" : res_area_list,\n",
    "        \"res_vol_km3\" : res_vol_list,\n",
    "        \"res_attr_source\" : res_attr_source_list,\n",
    "        \"res_attr_id\" : res_attr_id_list})\n",
    "    \n",
    "    return hydroalakes_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "plant_id_matched_hydrlakes_id = hydrolakes_manual_hydropower_reservoir_identification_func(dam_dataset=hydrolakes_point_data, hydropower_data=glohydrores, dem_data=DEM_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_id_matched_hydrlakes_id[\"res_dam_source\"] = \"HydroLakes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the glohydrores dataframe with the matched HydroLakes data\n",
    "row_indices = glohydrores.loc[glohydrores.plant_source_id.isin(plant_id_matched_hydrlakes_id.plant_source_id)].index\n",
    "glohydrores.loc[row_indices, [\"dam_name\", \"dam_height_m\", \"dam_height_source\", \"res_name\", \"river\", \"res_avg_depth_m\", \"res_area_km2\", \"res_vol_km3\", \"res_attr_source\", \"res_attr_id\", \"res_dam_source_id\", \"res_dam_source\"]] = plant_id_matched_hydrlakes_id[[\"dam_name\", \"dam_height_m\", \"dam_height_source\", \"res_name\", \"river\", \"res_avg_depth_m\", \"res_area_km2\", \"res_vol_km3\", \"res_attr_source\", \"res_attr_id\", \"res_dam_source_id\", \"res_dam_source\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shah0012/.conda/envs/python_env/lib/python3.7/site-packages/pandas/core/indexing.py:1835: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    }
   ],
   "source": [
    "glohydrores.to_excel(\"/home/shah0012/GloHydroRes_data/output_data/glohydrores_v1.xlsx\", sheet_name = \"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
